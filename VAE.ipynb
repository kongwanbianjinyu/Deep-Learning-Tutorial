{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+jFr9qXoHlaHaoCc0pksR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kongwanbianjinyu/Deep-Learning-Tutorial/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hUR7wdv0It_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b394c447-3de4-4c25-9adb-1a6251cd90d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 585 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 42.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 41.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 26.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --quiet \"pandas\" \"torch>=1.6, <1.9\" \"torchvision\" \"ipython[notebook]\" \"seaborn\" \"pytorch-lightning>=1.4\" \"torchmetrics>=0.6\" \"lightning-bolts\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "\n",
        "pl.seed_everything(1234)\n",
        "from torch import nn\n",
        "import torch\n",
        "from pl_bolts.models.autoencoders.components import (\n",
        "    resnet18_decoder,\n",
        "    resnet18_encoder,\n",
        ")\n",
        "from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FkSse2KI1gA",
        "outputId": "923e1fa4-8ad2-43d6-fa89-3eb6450e673e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kEdJMk3j1acv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(pl.LightningModule):\n",
        "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # encoder, decoder\n",
        "        self.encoder = resnet18_encoder(False, False)\n",
        "        self.decoder = resnet18_decoder(\n",
        "            latent_dim=latent_dim,\n",
        "            input_height=input_height,\n",
        "            first_conv=False,\n",
        "            maxpool1=False\n",
        "        )\n",
        "\n",
        "        # distribution parameters\n",
        "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
        "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
        "\n",
        "        # for the gaussian likelihood\n",
        "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
        "        scale = torch.exp(logscale)\n",
        "        mean = x_hat\n",
        "        dist = torch.distributions.Normal(mean, scale)\n",
        "\n",
        "        # measure prob of seeing image under p(x|z)\n",
        "        log_pxz = dist.log_prob(x)\n",
        "        return log_pxz.sum(dim=(1, 2, 3))\n",
        "\n",
        "    def kl_divergence(self, z, mu, std):\n",
        "        # --------------------------\n",
        "        # Monte carlo KL divergence\n",
        "        # --------------------------\n",
        "        # 1. define the first two probabilities (in this case Normal for both)\n",
        "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
        "        q = torch.distributions.Normal(mu, std)\n",
        "\n",
        "        # 2. get the probabilities from the equation\n",
        "        log_qzx = q.log_prob(z)\n",
        "        log_pz = p.log_prob(z)\n",
        "\n",
        "        # kl\n",
        "        kl = (log_qzx - log_pz)\n",
        "        kl = kl.sum(-1)\n",
        "        return kl\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "\n",
        "        # encode x to get the mu and variance parameters\n",
        "        x_encoded = self.encoder(x)\n",
        "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
        "\n",
        "        # sample z from q\n",
        "        std = torch.exp(log_var / 2)\n",
        "        q = torch.distributions.Normal(mu, std)\n",
        "        z = q.rsample()\n",
        "\n",
        "        # decoded\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        # reconstruction loss\n",
        "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
        "\n",
        "        # kl\n",
        "        kl = self.kl_divergence(z, mu, std)\n",
        "\n",
        "        # elbo\n",
        "        elbo = (kl - recon_loss)\n",
        "        elbo = elbo.mean()\n",
        "\n",
        "        self.log_dict({\n",
        "            'elbo': elbo,\n",
        "            'kl': kl.mean(),\n",
        "            'recon_loss': recon_loss.mean(),\n",
        "            'reconstruction': recon_loss.mean(),\n",
        "            'kl': kl.mean(),\n",
        "        })\n",
        "\n",
        "        return elbo"
      ],
      "metadata": {
        "id": "UgaUUSVvI0_5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow, figure\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "\n",
        "class ImageSampler(pl.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.img_size = None\n",
        "        self.num_preds = 16\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module, outputs):\n",
        "        figure(figsize=(8, 3), dpi=300)\n",
        "\n",
        "        # Z COMES FROM NORMAL(0, 1)\n",
        "        rand_v = torch.rand((self.num_preds, pl_module.hparams.latent_dim), device=pl_module.device)\n",
        "        p = torch.distributions.Normal(torch.zeros_like(rand_v), torch.zeros_like(rand_v))\n",
        "        z = p.rsample()\n",
        "\n",
        "        # SAMPLE IMAGES\n",
        "        with torch.no_grad():\n",
        "            pred = pl_module.decoder(z.to(pl_module.device)).cpu()\n",
        "\n",
        "        # UNDO DATA NORMALIZATION\n",
        "        normalize = cifar10_normalization()\n",
        "        mean, std = np.array(normalize.mean), np.array(normalize.std)\n",
        "        img = make_grid(pred).permute(1, 2, 0).numpy() * std + mean\n",
        "\n",
        "        # PLOT IMAGES\n",
        "        trainer.logger.experiment.add_image('img',torch.tensor(img).permute(2, 0, 1), global_step=trainer.global_step)"
      ],
      "metadata": {
        "id": "1vyrwWuUKR-O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.RandomCrop(32, padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cifar10_dm = CIFAR10DataModule(\n",
        "    data_dir=\".\",\n",
        "    batch_size=64,\n",
        "    num_workers=int(os.cpu_count() / 2),\n",
        ")"
      ],
      "metadata": {
        "id": "bh2TJOvE5y7f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "cifar10 = CIFAR10DataModule(data_dir='.')\n",
        "imagenet = ImagenetDataModule('.')\n",
        "\n",
        "sampler = ImageSampler()\n",
        "\n",
        "vae = VAE()\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=20, callbacks=[sampler])\n",
        "trainer.fit(vae,cifar10_dm)"
      ],
      "metadata": {
        "id": "eG6RebijKYfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "908f621d-129c-4e79-9a44-b4e925b4c014"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:115: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Missing logger folder: /content/lightning_logs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aa5d59d7e8a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcifar10_dm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_test_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_val_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dev_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfast_dev_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_dev_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mlimit_train_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m# teardown might access the stage so we reset it after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     def fit(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mscale_batch_size_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArguments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_batch_size\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mlr_find_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArguments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m         \"\"\"\n\u001b[1;32m   1106\u001b[0m         \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tune\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_setup_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_connected\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             rank_zero_warn(\n\u001b[0m\u001b[1;32m   1447\u001b[0m                 \u001b[0;34mf\"`.{fn}(ckpt_path=None)` was called without a model.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                 \u001b[0;34m\" The best model of the previous `fit` call will be used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0mstrategy_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy_output\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pl_bolts/datamodules/vision_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m\"\"\"Creates train, val, and test dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fit\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtrain_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mval_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_transforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10DataModule' object has no attribute 'train_transforms'"
          ]
        }
      ]
    }
  ]
}